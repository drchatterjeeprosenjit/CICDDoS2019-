{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-22T19:56:19.497350Z","iopub.execute_input":"2023-02-22T19:56:19.498393Z","iopub.status.idle":"2023-02-22T19:56:19.520005Z","shell.execute_reply.started":"2023-02-22T19:56:19.498345Z","shell.execute_reply":"2023-02-22T19:56:19.519127Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/UnderSampledDataUsingRepeatedEnn/undersampled_using_repeated_enn.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('/kaggle/input/UnderSampledDataUsingRepeatedEnn/undersampled_using_repeated_enn.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T19:56:21.252588Z","iopub.execute_input":"2023-02-22T19:56:21.253184Z","iopub.status.idle":"2023-02-22T19:56:24.465141Z","shell.execute_reply.started":"2023-02-22T19:56:21.253134Z","shell.execute_reply":"2023-02-22T19:56:24.464155Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Using Simple LSTM**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.utils import np_utils\n\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = np_utils.to_categorical(labels)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(labels.shape[1], activation=\"softmax\"))\n\n# Compile the model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n\n# Evaluate the model\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:46:23.353115Z","iopub.execute_input":"2023-02-22T17:46:23.353758Z","iopub.status.idle":"2023-02-22T17:50:09.066265Z","shell.execute_reply.started":"2023-02-22T17:46:23.353720Z","shell.execute_reply":"2023-02-22T17:50:09.065041Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/5\n2775/2775 [==============================] - 37s 11ms/step - loss: 0.3209 - accuracy: 0.8852 - val_loss: 0.1922 - val_accuracy: 0.9243\nEpoch 2/5\n2775/2775 [==============================] - 30s 11ms/step - loss: 0.2000 - accuracy: 0.9237 - val_loss: 0.1742 - val_accuracy: 0.9326\nEpoch 3/5\n2775/2775 [==============================] - 30s 11ms/step - loss: 0.1840 - accuracy: 0.9296 - val_loss: 0.1860 - val_accuracy: 0.9299\nEpoch 4/5\n2775/2775 [==============================] - 30s 11ms/step - loss: 0.1769 - accuracy: 0.9311 - val_loss: 0.1745 - val_accuracy: 0.9317\nEpoch 5/5\n2775/2775 [==============================] - 30s 11ms/step - loss: 0.1695 - accuracy: 0.9340 - val_loss: 0.1646 - val_accuracy: 0.9355\nTest loss: 0.16461579501628876\nTest accuracy: 0.9355107545852661\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Use Optuna to Optimize Hyper Parameter LSTM**","metadata":{}},{"cell_type":"code","source":"!pip install optuna\n!pip install -q keras","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:50:09.068567Z","iopub.execute_input":"2023-02-22T17:50:09.069015Z","iopub.status.idle":"2023-02-22T17:50:30.333621Z","shell.execute_reply.started":"2023-02-22T17:50:09.068977Z","shell.execute_reply":"2023-02-22T17:50:30.332363Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (3.1.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (6.7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (23.0)\nRequirement already satisfied: cmaes>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.9.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.9.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.41)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.64.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (6.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (5.10.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (6.0.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.8.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.utils import np_utils\nimport optuna\nfrom keras import optimizers\n\n\n# Define the objective function to be optimized\ndef objective(trial):\n    # Define the hyperparameters to be tuned\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n    n_units = trial.suggest_int(\"n_units\", 50, 200)\n    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n    learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-5, 1e-2)\n\n    # Extract the features and label\n    features = df.drop(\"Label\", axis=1).values\n    labels = df[\"Label\"].values\n\n    # Encode the labels to numeric values\n    encoder = LabelEncoder()\n    labels = encoder.fit_transform(labels)\n    labels = np_utils.to_categorical(labels)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n\n    # Reshape the data for LSTM\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    # Define the LSTM model\n    model = Sequential()\n    for i in range(n_layers):\n        if i == 0:\n            model.add(LSTM(n_units, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n        elif i == n_layers - 1:\n            model.add(LSTM(n_units))\n        else:\n            model.add(LSTM(n_units, return_sequences=True))\n        model.add(Dropout(dropout_rate))\n\n    model.add(Dense(labels.shape[1], activation=\"softmax\"))\n\n    # Compile the model with the learning rate\n    optimizer = keras.optimizers.Adam(lr=learning_rate)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n\n    # Train the model\n    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n\n    # Evaluate the model\n    score = model.evaluate(X_test, y_test, verbose=0)\n\n    return score[1]  # Return the validation accuracy as the objective value\n\n# Define the study to optimize the objective function\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=10)  # Run 100 trials to optimize the hyperparameters\n\n# Print the best hyperparameters and the corresponding accuracy\nbest_trial = study.best_trial\nprint(\"Best trial:\")\nprint(\"  Value: {}\".format(best_trial.value))\nprint(\"  Params: \")\nfor key, value in best_trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:50:30.336713Z","iopub.execute_input":"2023-02-22T17:50:30.337158Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2023-02-22 17:50:30,598]\u001b[0m A new study created in memory with name: no-name-dc3a7fc2-903f-406b-a937-bdad7b006409\u001b[0m\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n/opt/conda/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(Adam, self).__init__(name, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import np_utils\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = np_utils.to_categorical(labels)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n\n# Define the autoencoder model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(features.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(features.shape[1], activation='sigmoid'))\n\n# Compile the autoencoder model\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Train the autoencoder model\nmodel.fit(X_train, X_train, epochs=50, batch_size=64, shuffle=True, validation_data=(X_test, X_test))\n\n# Extract the encoded features from the autoencoder model\nencoder_model = Sequential()\nencoder_model.add(model.layers[0])\nencoder_model.add(model.layers[1])\nencoder_model.add(model.layers[2])\nencoded_train = encoder_model.predict(X_train)\nencoded_test = encoder_model.predict(X_test)\n\n# Define the LSTM model with encoded features\nlstm_model = Sequential()\nlstm_model.add(LSTM(100, input_shape=(encoded_train.shape[1], encoded_train.shape[2]), return_sequences=True))\nlstm_model.add(Dropout(0.2))\nlstm_model.add(LSTM(100))\nlstm_model.add(Dropout(0.2))\nlstm_model.add(Dense(labels.shape[1], activation=\"softmax\"))\n\n# Compile the LSTM model\nlstm_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Train the LSTM model with encoded features\nlstm_model.fit(encoded_train, y_train, epochs=5, batch_size=64, validation_data=(encoded_test, y_test))\n\n# Evaluate the LSTM model with encoded features\nscore = lstm_model.evaluate(encoded_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T19:56:30.285773Z","iopub.execute_input":"2023-02-22T19:56:30.286133Z","iopub.status.idle":"2023-02-22T20:05:05.398654Z","shell.execute_reply.started":"2023-02-22T19:56:30.286101Z","shell.execute_reply":"2023-02-22T20:05:05.397391Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/50\n2775/2775 [==============================] - 11s 3ms/step - loss: nan - val_loss: nan\nEpoch 2/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 3/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 4/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 5/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 6/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 7/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 8/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 9/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 10/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 11/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 12/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 13/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 14/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 15/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 16/50\n2775/2775 [==============================] - 10s 4ms/step - loss: nan - val_loss: nan\nEpoch 17/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 18/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 19/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 20/50\n2775/2775 [==============================] - 11s 4ms/step - loss: nan - val_loss: nan\nEpoch 21/50\n2775/2775 [==============================] - 11s 4ms/step - loss: nan - val_loss: nan\nEpoch 22/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 23/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 24/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 25/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 26/50\n2775/2775 [==============================] - 8s 3ms/step - loss: nan - val_loss: nan\nEpoch 27/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 28/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 29/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 30/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 31/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 32/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 33/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 34/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 35/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 36/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 37/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 38/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 39/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 40/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 41/50\n2775/2775 [==============================] - 10s 4ms/step - loss: nan - val_loss: nan\nEpoch 42/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 43/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 44/50\n2775/2775 [==============================] - 10s 4ms/step - loss: nan - val_loss: nan\nEpoch 45/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 46/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 47/50\n2775/2775 [==============================] - 10s 4ms/step - loss: nan - val_loss: nan\nEpoch 48/50\n2775/2775 [==============================] - 10s 4ms/step - loss: nan - val_loss: nan\nEpoch 49/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\nEpoch 50/50\n2775/2775 [==============================] - 9s 3ms/step - loss: nan - val_loss: nan\n5550/5550 [==============================] - 7s 1ms/step\n1388/1388 [==============================] - 2s 1ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/442126167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Define the LSTM model with encoded features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'LSTM' is not defined"],"ename":"NameError","evalue":"name 'LSTM' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}