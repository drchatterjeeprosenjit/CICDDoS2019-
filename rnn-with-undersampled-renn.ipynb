{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-23T23:04:39.394895Z","iopub.execute_input":"2023-02-23T23:04:39.395706Z","iopub.status.idle":"2023-02-23T23:04:39.433757Z","shell.execute_reply.started":"2023-02-23T23:04:39.395600Z","shell.execute_reply":"2023-02-23T23:04:39.432445Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/undersampledrenn/Repeated_Edited_Nearest_Neighbors_Under-sampled_Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv('/kaggle/input/undersampledrenn/Repeated_Edited_Nearest_Neighbors_Under-sampled_Dataset.csv')\ndf = df[df['Label'] != 'BENIGN']","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:56:36.482600Z","iopub.execute_input":"2023-02-23T17:56:36.482950Z","iopub.status.idle":"2023-02-23T17:56:38.029984Z","shell.execute_reply.started":"2023-02-23T17:56:36.482919Z","shell.execute_reply":"2023-02-23T17:56:38.029000Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Startified K-fold","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, Dropout\nfrom keras.utils import np_utils\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = np_utils.to_categorical(labels)\n\n# Define the number of folds for stratified K-fold cross-validation\nk_folds = 5\n\n# Define the RNN model\nmodel = Sequential()\nmodel.add(SimpleRNN(100, input_shape=(features.shape[1], 1), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(SimpleRNN(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(labels.shape[1], activation=\"softmax\"))\n\n# Compile the model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\ntrain_acc_list = []\ntrain_loss_list = []\ntest_acc_list = []\ntest_loss_list = []\n# Perform stratified K-fold cross-validation\nskf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\nfor i, (train_index, test_index) in enumerate(skf.split(features, labels.argmax(1))):\n    print(\"Fold\", i+1)\n\n    # Split the data into training and testing sets\n    X_train, X_test = features[train_index], features[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]\n\n    # Reshape the data for RNN\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    # Train the model\n    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n\n    # Evaluate the model\n    train_loss, train_accuracy = model.evaluate(X_train, y_train)\n\n    # Evaluate the model on the test set\n    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n    \n    train_acc_list.append(train_accuracy)\n    train_loss_list.append(train_loss)\n    test_acc_list.append(test_accuracy)\n    test_loss_list.append(test_loss)\n\nprint(\"Average Training Accuracy: {}\".format(np.mean(train_acc_list)))\nprint(\"Average Training Loss: {}\".format(np.mean(train_loss_list)))\nprint(\"Average Testing Accuracy: {}\".format(np.mean(test_acc_list)))\nprint(\"Average Testing Loss: {}\".format(np.mean(test_loss_list)))","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:56:42.668175Z","iopub.execute_input":"2023-02-23T17:56:42.668577Z","iopub.status.idle":"2023-02-23T20:20:22.952167Z","shell.execute_reply.started":"2023-02-23T17:56:42.668533Z","shell.execute_reply":"2023-02-23T20:20:22.951200Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fold 1\nEpoch 1/5\n5000/5000 [==============================] - 337s 67ms/step - loss: 0.5459 - accuracy: 0.7979 - val_loss: 0.2899 - val_accuracy: 0.8920\nEpoch 2/5\n5000/5000 [==============================] - 334s 67ms/step - loss: 0.3103 - accuracy: 0.8870 - val_loss: 0.2671 - val_accuracy: 0.8970\nEpoch 3/5\n5000/5000 [==============================] - 335s 67ms/step - loss: 0.3044 - accuracy: 0.8886 - val_loss: 0.3386 - val_accuracy: 0.8834\nEpoch 4/5\n5000/5000 [==============================] - 334s 67ms/step - loss: 0.2911 - accuracy: 0.8923 - val_loss: 0.3068 - val_accuracy: 0.8787\nEpoch 5/5\n5000/5000 [==============================] - 336s 67ms/step - loss: 0.3430 - accuracy: 0.8749 - val_loss: 0.2609 - val_accuracy: 0.9014\n5000/5000 [==============================] - 42s 8ms/step - loss: 0.2643 - accuracy: 0.9016\n1250/1250 [==============================] - 10s 8ms/step - loss: 0.2609 - accuracy: 0.9014\nFold 2\nEpoch 1/5\n5000/5000 [==============================] - 333s 67ms/step - loss: 0.3100 - accuracy: 0.8882 - val_loss: 0.2736 - val_accuracy: 0.8981\nEpoch 2/5\n5000/5000 [==============================] - 334s 67ms/step - loss: 0.3019 - accuracy: 0.8867 - val_loss: 0.2692 - val_accuracy: 0.8917\nEpoch 3/5\n5000/5000 [==============================] - 335s 67ms/step - loss: 0.3249 - accuracy: 0.8810 - val_loss: 0.2828 - val_accuracy: 0.8929\nEpoch 4/5\n5000/5000 [==============================] - 331s 66ms/step - loss: 0.3041 - accuracy: 0.8888 - val_loss: 0.2808 - val_accuracy: 0.9014\nEpoch 5/5\n5000/5000 [==============================] - 330s 66ms/step - loss: 0.3052 - accuracy: 0.8879 - val_loss: 0.2760 - val_accuracy: 0.8839\n5000/5000 [==============================] - 41s 8ms/step - loss: 0.2839 - accuracy: 0.8807\n1250/1250 [==============================] - 11s 8ms/step - loss: 0.2760 - accuracy: 0.8839\nFold 3\nEpoch 1/5\n5000/5000 [==============================] - 331s 66ms/step - loss: 0.3216 - accuracy: 0.8801 - val_loss: 0.3161 - val_accuracy: 0.8802\nEpoch 2/5\n5000/5000 [==============================] - 331s 66ms/step - loss: 0.3017 - accuracy: 0.8883 - val_loss: 0.2986 - val_accuracy: 0.8767\nEpoch 3/5\n5000/5000 [==============================] - 331s 66ms/step - loss: 0.3381 - accuracy: 0.8734 - val_loss: 0.4112 - val_accuracy: 0.8378\nEpoch 4/5\n5000/5000 [==============================] - 332s 66ms/step - loss: 0.3141 - accuracy: 0.8836 - val_loss: 0.2596 - val_accuracy: 0.9038\nEpoch 5/5\n5000/5000 [==============================] - 330s 66ms/step - loss: 0.3074 - accuracy: 0.8865 - val_loss: 0.3055 - val_accuracy: 0.8917\n5000/5000 [==============================] - 41s 8ms/step - loss: 0.2983 - accuracy: 0.8948\n1250/1250 [==============================] - 10s 8ms/step - loss: 0.3055 - accuracy: 0.8917\nFold 4\nEpoch 1/5\n5000/5000 [==============================] - 333s 67ms/step - loss: 0.3088 - accuracy: 0.8878 - val_loss: 0.3007 - val_accuracy: 0.8866\nEpoch 2/5\n5000/5000 [==============================] - 333s 67ms/step - loss: 0.3087 - accuracy: 0.8856 - val_loss: 0.2567 - val_accuracy: 0.9063\nEpoch 3/5\n5000/5000 [==============================] - 332s 66ms/step - loss: 0.2966 - accuracy: 0.8896 - val_loss: 0.2761 - val_accuracy: 0.8961\nEpoch 4/5\n5000/5000 [==============================] - 330s 66ms/step - loss: 0.3079 - accuracy: 0.8861 - val_loss: 0.2764 - val_accuracy: 0.9040\nEpoch 5/5\n5000/5000 [==============================] - 332s 66ms/step - loss: 0.3330 - accuracy: 0.8754 - val_loss: 0.3147 - val_accuracy: 0.8923\n5000/5000 [==============================] - 41s 8ms/step - loss: 0.3128 - accuracy: 0.8912\n1250/1250 [==============================] - 10s 8ms/step - loss: 0.3147 - accuracy: 0.8923\nFold 5\nEpoch 1/5\n5000/5000 [==============================] - 333s 67ms/step - loss: 0.3286 - accuracy: 0.8799 - val_loss: 0.2750 - val_accuracy: 0.8996\nEpoch 2/5\n5000/5000 [==============================] - 334s 67ms/step - loss: 0.3290 - accuracy: 0.8808 - val_loss: 0.2840 - val_accuracy: 0.8944\nEpoch 3/5\n5000/5000 [==============================] - 334s 67ms/step - loss: 0.3403 - accuracy: 0.8727 - val_loss: 0.3327 - val_accuracy: 0.8781\nEpoch 4/5\n5000/5000 [==============================] - 333s 67ms/step - loss: 0.3470 - accuracy: 0.8707 - val_loss: 0.3055 - val_accuracy: 0.8876\nEpoch 5/5\n5000/5000 [==============================] - 333s 67ms/step - loss: 0.3582 - accuracy: 0.8706 - val_loss: 0.3046 - val_accuracy: 0.8907\n5000/5000 [==============================] - 41s 8ms/step - loss: 0.3040 - accuracy: 0.8912\n1250/1250 [==============================] - 10s 8ms/step - loss: 0.3046 - accuracy: 0.8907\nAverage Training Accuracy: 0.8918934464454651\nAverage Training Loss: 0.2926622986793518\nAverage Testing Accuracy: 0.8919909358024597\nAverage Testing Loss: 0.2923396348953247\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Just the simple model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, Dropout\nfrom keras.utils import np_utils\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = np_utils.to_categorical(labels)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n\n# Reshape the data for RNN\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Define the RNN model\nmodel = Sequential()\nmodel.add(SimpleRNN(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(SimpleRNN(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(labels.shape[1], activation=\"softmax\"))\n\n# Compile the model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n\n# Evaluate the model\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T20:39:12.868959Z","iopub.execute_input":"2023-02-19T20:39:12.869403Z","iopub.status.idle":"2023-02-19T20:49:53.192974Z","shell.execute_reply.started":"2023-02-19T20:39:12.869365Z","shell.execute_reply":"2023-02-19T20:49:53.191951Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/5\n2775/2775 [==============================] - 127s 45ms/step - loss: 0.5717 - accuracy: 0.7953 - val_loss: 0.3021 - val_accuracy: 0.8789\nEpoch 2/5\n2775/2775 [==============================] - 125s 45ms/step - loss: 0.3463 - accuracy: 0.8653 - val_loss: 0.3123 - val_accuracy: 0.8737\nEpoch 3/5\n2775/2775 [==============================] - 125s 45ms/step - loss: 0.3752 - accuracy: 0.8568 - val_loss: 0.3205 - val_accuracy: 0.8721\nEpoch 4/5\n2775/2775 [==============================] - 125s 45ms/step - loss: 0.3330 - accuracy: 0.8697 - val_loss: 0.3637 - val_accuracy: 0.8620\nEpoch 5/5\n2775/2775 [==============================] - 124s 45ms/step - loss: 0.3665 - accuracy: 0.8611 - val_loss: 0.3181 - val_accuracy: 0.8746\nTest loss: 0.3181447386741638\nTest accuracy: 0.8746029734611511\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Use optuna","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}