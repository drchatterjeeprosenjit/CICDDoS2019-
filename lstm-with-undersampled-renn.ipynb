{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-24T07:59:04.990922Z","iopub.execute_input":"2023-02-24T07:59:04.991285Z","iopub.status.idle":"2023-02-24T07:59:05.020816Z","shell.execute_reply.started":"2023-02-24T07:59:04.991205Z","shell.execute_reply":"2023-02-24T07:59:05.019876Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/undersampledrenn/Repeated_Edited_Nearest_Neighbors_Under-sampled_Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv('/kaggle/input/undersampledrenn/Repeated_Edited_Nearest_Neighbors_Under-sampled_Dataset.csv')\ndf = df[df['Label'] != 'BENIGN']","metadata":{"execution":{"iopub.status.busy":"2023-02-24T07:59:05.022742Z","iopub.execute_input":"2023-02-24T07:59:05.023256Z","iopub.status.idle":"2023-02-24T07:59:06.575921Z","shell.execute_reply.started":"2023-02-24T07:59:05.023205Z","shell.execute_reply":"2023-02-24T07:59:06.574891Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-24T06:23:33.170071Z","iopub.execute_input":"2023-02-24T06:23:33.170526Z","iopub.status.idle":"2023-02-24T06:23:33.183574Z","shell.execute_reply.started":"2023-02-24T06:23:33.170485Z","shell.execute_reply":"2023-02-24T06:23:33.182458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From The paper\nBuild a LSTM Neural Network with the following description: three LSTM layers with 128 unit and sigmoid as \nactivation function, three dropout layers, and a dense \nlayer with tanh function. Use categorical_crossentropy as loss function and Root \nMean Square Propagation (rmsprop) as an optimizer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout,BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.optimizers import Adam\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dropout, Dense\n\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = to_categorical(labels)\n\n\n# Define the LSTM model\n# define the model\nmodel = Sequential()\nmodel.add(LSTM(units=128, activation='sigmoid', return_sequences=True, input_shape=(features.shape[1], 1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=128, activation='sigmoid', return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=128, activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(labels.shape[1], activation='softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n# print the model summary\nmodel.summary()\n\n# Define the number of folds\nn_splits = 5\n\n# Initialize lists to store results\nacc_scores = []\nloss_scores = []\n\n# Split the data into training and testing sets\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\nfor train_index, test_index in skf.split(features,labels.argmax(1)):\n    X_train, X_test = features[train_index], features[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]\n\n    # Reshape the data for LSTM\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n\n\n    # Train the model\n    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n    # Evaluate the model\n    score = model.evaluate(X_test, y_test, verbose=0)\n    loss_scores.append(score[0])\n    acc_scores.append(score[1])\n\n# Print the average accuracy and loss\nprint(f'Average accuracy across {n_splits} folds: {np.mean(acc_scores):.2f}')\nprint(f'Average loss across {n_splits} folds: {np.mean(loss_scores):.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-24T07:59:06.577535Z","iopub.execute_input":"2023-02-24T07:59:06.577917Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 42, 128)           66560     \n                                                                 \n dropout (Dropout)           (None, 42, 128)           0         \n                                                                 \n lstm_1 (LSTM)               (None, 42, 128)           131584    \n                                                                 \n dropout_1 (Dropout)         (None, 42, 128)           0         \n                                                                 \n lstm_2 (LSTM)               (None, 128)               131584    \n                                                                 \n dropout_2 (Dropout)         (None, 128)               0         \n                                                                 \n dense (Dense)               (None, 11)                1419      \n                                                                 \n=================================================================\nTotal params: 331,147\nTrainable params: 331,147\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/5\n5000/5000 [==============================] - 1084s 216ms/step - loss: 0.6547 - accuracy: 0.7691 - val_loss: 0.5418 - val_accuracy: 0.8064\nEpoch 2/5\n5000/5000 [==============================] - 1082s 216ms/step - loss: 0.2545 - accuracy: 0.9076 - val_loss: 0.2084 - val_accuracy: 0.9222\nEpoch 5/5\n3734/5000 [=====================>........] - ETA: 4:26 - loss: 0.2308 - accuracy: 0.9163","output_type":"stream"}]},{"cell_type":"markdown","source":"# Better","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout,BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.optimizers import Adam\n\n\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = to_categorical(labels)\n\n# Define the number of folds\nn_splits = 5\n\n# Initialize lists to store results\nacc_scores = []\nloss_scores = []\n\n\n# Define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(features.shape[1], 1), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(labels.shape[1], activation=\"softmax\"))\n\n# Compile the model\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n\n# Split the data into training and testing sets\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\nfor train_index, test_index in skf.split(features,labels.argmax(1)):\n    X_train, X_test = features[train_index], features[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]\n\n    # Reshape the data for LSTM\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    \n\n    # Train the model\n    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n    # Evaluate the model\n    score = model.evaluate(X_test, y_test, verbose=0)\n    loss_scores.append(score[0])\n    acc_scores.append(score[1])\n\n# Print the average accuracy and loss\nprint(f'Average accuracy across {n_splits} folds: {np.mean(acc_scores):.2f}')\nprint(f'Average loss across {n_splits} folds: {np.mean(loss_scores):.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T22:36:44.577893Z","iopub.execute_input":"2023-02-23T22:36:44.578582Z","iopub.status.idle":"2023-02-23T22:57:53.339005Z","shell.execute_reply.started":"2023-02-23T22:36:44.578545Z","shell.execute_reply":"2023-02-23T22:57:53.337994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout,BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.optimizers import Adam\n\n\n\n# Extract the features and label\nfeatures = df.drop(\"Label\", axis=1).values\nlabels = df[\"Label\"].values\n\n# Encode the labels to numeric values\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\nlabels = to_categorical(labels)\n\n# Define the number of folds\nn_splits = 5\n\n# Initialize lists to store results\nacc_scores = []\nloss_scores = []\n\n# Split the data into training and testing sets\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\nfor train_index, test_index in skf.split(features,labels.argmax(1)):\n    X_train, X_test = features[train_index], features[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]\n\n    # Reshape the data for LSTM\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    model = Sequential()\n    model.add(LSTM(128,input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dense(64,activation='relu'))\n    model.add(Dense(64,activation='relu'))\n    model.add((Dense(labels.shape[1], activation='softmax')))\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-4), metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n\n    # Evaluate the model\n    score = model.evaluate(X_test, y_test, verbose=0)\n    loss_scores.append(score[0])\n    acc_scores.append(score[1])\n\n# Print the average accuracy and loss\nprint(f'Average accuracy across {n_splits} folds: {np.mean(acc_scores):.2f}')\nprint(f'Average loss across {n_splits} folds: {np.mean(loss_scores):.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T15:25:19.919862Z","iopub.execute_input":"2023-02-23T15:25:19.920336Z","iopub.status.idle":"2023-02-23T15:34:56.465975Z","shell.execute_reply.started":"2023-02-23T15:25:19.920287Z","shell.execute_reply":"2023-02-23T15:34:56.464848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}